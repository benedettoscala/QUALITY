# QUALITY: Quantifying the Quality, Fairness, and Performance of Large Language Model-Generated Data
## Introduction
This project is conducted as part of the **Software Engineering for AI** course. The objective is to perform an empirical study on the quality of tabular data generated by Large Language Models (LLMs), specifically GPT-4o, and to evaluate their usability in various dimensions. We have recreated the well-known **German Credit** dataset using three prompt engineering techniques: *0-Shot*, *1-Shot*, and *2-Shot*.

Our investigation revolves around three main **research questions**. Firstly, we look at structural metrics to evaluate the generated data. These metrics include *Uniqueness*, *Readability*, *Consistency*, and *Completeness*.

Secondly, we focus on performance metrics. We measure how well machine learning models perform when trained on the generated data. Specifically, we look at the *F1-score* and *Accuracy* to understand the effectiveness of these datasets in predictive tasks.

Lastly, we examine fairness metrics to ensure that the generated data does not introduce or perpetuate bias. We use metrics like Equal *Opportunity Difference (EOD)*, *Average Odds Difference (AOD)*, and *Statistical Parity Difference (SPD)* to assess how fair the data is across different demographic groups.

## Usage

## Repository Structure
- `data/`: Contains the original German Credit dataset and the datasets generated using the different prompt engineering techniques.
- `notebooks/`: Jupyter notebooks used for data generation, analysis, and visualization.
- `documents`: Contains the paper of our work in which you can find the Methodology and Results, and a presentation pdf file.

## Authors
- [Benedetto Scala](https://github.com/benedettoscala)
- [Leopoldo Todisco](https://github.com/leotodisco)
- [Carlo Venditto](https://github.com/carlovend)
